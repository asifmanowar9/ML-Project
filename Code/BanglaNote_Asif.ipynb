{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOowOQ3t4saK0G7yhvzbbFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifmanowar9/BanglaTaka/blob/main/Code/BanglaNote_Asif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz8KLqsJ1vxW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9014178b"
      },
      "source": [
        "# Task\n",
        "Build and evaluate a Convolutional Neural Network (CNN) model to classify banknote images, using the dataset from the Google Drive folder located at 'https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA'. The task involves accessing the data, preparing it, splitting it into training, validation, and test sets, building and training the CNN model, evaluating its performance with relevant metrics, and providing a summary of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1338d51"
      },
      "source": [
        "## Access Dataset from Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to access the dataset folder and list its contents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "215df033"
      },
      "source": [
        "### Step 1: Mount Google Drive and Access Dataset Folder\n",
        "\n",
        "To access the dataset, you'll first need to mount your Google Drive to this Colab environment.\n",
        "\n",
        "**Important Note:** The provided Google Drive link `https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA` points to a shared folder. To make it accessible from your mounted Google Drive, you **must create a shortcut to this shared folder in your 'My Drive'**.\n",
        "\n",
        "For example, if you create a shortcut named `DSA_Project_Folder` in your 'My Drive', its path after mounting will typically be `/content/drive/MyDrive/DSA_Project_Folder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f40e52e1"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide the Python code to mount Google Drive, which is the first step in accessing the dataset as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4227041",
        "outputId": "859966b6-57ac-44ac-bce7-6dc1169f5a2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84043e4a"
      },
      "source": [
        "### Step 2: List Contents of the Shared Dataset Folder\n",
        "\n",
        "To confirm that you can access the dataset and understand its structure, you need to list the contents of the shared Google Drive folder. Remember, you should have created a shortcut to the provided shared folder (`https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA`) in your 'My Drive'.\n",
        "\n",
        "Replace `Your_Shortcut_Name` in the code below with the actual name of the shortcut you created (e.g., `DSA_Project_Folder`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8170281e"
      },
      "source": [
        "**Reasoning**:\n",
        "To allow the user to list the contents of the mounted Google Drive folder, I will provide a Python code block with a placeholder for their specific shortcut name. This directly addresses the second instruction of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d02c7c1",
        "outputId": "4edfc033-668a-4c00-fda8-b3a351105ccf"
      },
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/aa/dataset' # Replace 'Your_Shortcut_Name' with the actual name of your shortcut\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    print(f\"Listing contents of '{drive_path}':\")\n",
        "    for item in os.listdir(drive_path):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"Error: The path '{drive_path}' does not exist. Please check your shortcut name and path.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of '/content/drive/MyDrive/aa/dataset':\n",
            "100\n",
            "200\n",
            "500\n",
            "1000\n",
            "50\n",
            "2\n",
            "5\n",
            "10\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae6af7dc"
      },
      "source": [
        "## Verify and List Contents of Shared Dataset Folder\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the Google Drive shortcut path is correctly set and list the contents of the folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e043ee0",
        "outputId": "28f8e94c-0d9a-49e4-eda6-fda77ce937f4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Load the dataset using image_dataset_from_directory\n",
        "# Ensure drive_path is correctly set from the previous step (e.g., '/content/drive/MyDrive/DSA_Project_Folder')\n",
        "# If drive_path was not updated, it will still point to the placeholder\n",
        "# We will use the last known value of drive_path from the kernel state.\n",
        "# drive_path was defined in cell_id: 4d02c7c1\n",
        "\n",
        "# Re-evaluate drive_path from the previous execution to ensure we use the potentially updated value.\n",
        "# If the user has not updated it yet, they will get an error here, which is expected as per instructions.\n",
        "# For demonstration, we use the value that caused the previous error. User needs to fix this manually.\n",
        "# A robust solution would involve re-running the previous cell to get the updated value.\n",
        "\n",
        "# Assuming the user has manually updated drive_path in the previous cell and re-run it\n",
        "# For the purpose of generating the next step, I'll use a placeholder or assume it's set correctly.\n",
        "# In a real interactive session, I would expect drive_path to be correctly updated by the user.\n",
        "\n",
        "# Using the variable from the kernel state for this demonstration step generation\n",
        "# In actual execution, if user fixes the previous cell, this will pick up the correct path.\n",
        "# If the previous cell was not re-run, drive_path will still be '/content/drive/MyDrive/Your_Shortcut_Name'\n",
        "\n",
        "# Let's add a check and a comment for the user to be aware.\n",
        "\n",
        "if 'drive_path' not in locals() or drive_path == '/content/drive/MyDrive/Your_Shortcut_Name':\n",
        "    print(\"Warning: 'drive_path' variable might still be set to the placeholder. Please ensure you have updated and re-run the previous cell (cell_id: 4d02c7c1) with your actual shortcut name.\")\n",
        "    # Attempt to load, it might fail if path is incorrect.\n",
        "    dataset_root = drive_path\n",
        "else:\n",
        "    dataset_root = drive_path\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # Print class names and number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "\n",
        "    # Calculate total number of images (approximate, as it depends on batch size and dataset size)\n",
        "    total_images = tf.data.experimental.cardinality(raw_train_ds).numpy() * batch_size\n",
        "    print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # Preprocessing function for normalization\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image / 255.0, tf.float32) # Normalize pixel values to [0, 1]\n",
        "        return image, label\n",
        "\n",
        "    # Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # Take one batch to verify preprocessing (optional)\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"Shape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'drive_path' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'drive_path' variable might still be set to the placeholder. Please ensure you have updated and re-run the previous cell (cell_id: 4d02c7c1) with your actual shortcut name.\n",
            "Attempting to load dataset from: /content/drive/MyDrive/Your_Shortcut_Name\n",
            "Error loading dataset: Could not find directory /content/drive/MyDrive/Your_Shortcut_Name\n",
            "Please ensure the 'drive_path' is correct and points to a directory containing image subfolders (classes).\n",
            "Example: '/content/drive/MyDrive/DSA_Project_Folder'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7584f8af"
      },
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Load the banknote images and their corresponding labels from the Google Drive folder, perform initial exploration, and preprocess the images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f975b232"
      },
      "source": [
        "**Reasoning**:\n",
        "The user provided detailed instructions for loading, exploring, and preprocessing the image dataset. I will generate a code block that imports TensorFlow, defines image dimensions and batch size, sets the corrected dataset root path, loads the dataset, prints class names, defines and applies a normalization function, and verifies the preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1833e882",
        "outputId": "d9a5997f-e6c5-4c72-ece2-27137ce4f059"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Set the dataset_root variable to the confirmed path\n",
        "dataset_root = '/content/drive/MyDrive/aa/dataset'\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    # 3. Load the image dataset using image_dataset_from_directory\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # 4. Print class names and approximate number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    # tf.data.experimental.cardinality returns the number of batches\n",
        "    # Multiply by batch_size to get approximate total images, as some batches might be smaller\n",
        "    total_batches = tf.data.experimental.cardinality(raw_train_ds).numpy()\n",
        "    if total_batches == tf.data.AUTOTUNE.numpy():\n",
        "        print(\"Total number of batches could not be determined. Dataset size might be very large.\")\n",
        "    else:\n",
        "        total_images = total_batches * batch_size\n",
        "        print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # 5. Preprocessing function for normalization\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image / 255.0, tf.float32) # Normalize pixel values to [0, 1]\n",
        "        return image, label\n",
        "\n",
        "    # 6. Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # 7. Verify preprocessing by taking one batch\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"\\nShape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: /content/drive/MyDrive/aa/dataset\n",
            "Found 10634 files belonging to 9 classes.\n",
            "\n",
            "Classes found: ['10', '100', '1000', '2', '20', '200', '5', '50', '500']\n",
            "Number of classes: 9\n",
            "Error loading dataset: 'int' object has no attribute 'numpy'\n",
            "Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\n",
            "Example: '/content/drive/MyDrive/DSA_Project_Folder'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4f3198"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `tf.data.experimental.cardinality` unexpectedly returned an `int` in some context, causing an `AttributeError` when `.numpy()` was called on it. I will correct the calculation of `total_images` by first assigning the result of `tf.data.experimental.cardinality` to a variable, and then conditionally calling `.numpy()` on it only if it's not `tf.data.AUTOTUNE` (which is a tensor itself). This ensures proper handling of both `tf.Tensor` and potential `int` return types.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e16b8f",
        "outputId": "eee8c2a4-4e1c-43d6-8c81-5a4460b3ba49"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Set the dataset_root variable to the confirmed path\n",
        "dataset_root = '/content/drive/MyDrive/aa/dataset'\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    # 3. Load the image dataset using image_dataset_from_directory\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # 4. Print class names and approximate number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    # Corrected: Handle cardinality carefully\n",
        "    total_batches_cardinality = tf.data.experimental.cardinality(raw_train_ds)\n",
        "    if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "        print(\"Total number of batches could not be determined (tf.data.AUTOTUNE). Dataset size might be very large.\")\n",
        "    else:\n",
        "        # Ensure we call .numpy() only if it's a TensorFlow Tensor\n",
        "        total_batches = total_batches_cardinality.numpy() if hasattr(total_batches_cardinality, 'numpy') else total_batches_cardinality\n",
        "        total_images = total_batches * batch_size\n",
        "        print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # 5. Preprocessing function for normalization\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image / 255.0, tf.float32) # Normalize pixel values to [0, 1]\n",
        "        return image, label\n",
        "\n",
        "    # 6. Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # 7. Verify preprocessing by taking one batch\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"\\nShape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: /content/drive/MyDrive/aa/dataset\n",
            "Found 10634 files belonging to 9 classes.\n",
            "\n",
            "Classes found: ['10', '100', '1000', '2', '20', '200', '5', '50', '500']\n",
            "Number of classes: 9\n",
            "Approximate total number of images in the dataset: 10656\n",
            "Error loading dataset: in user code:\n",
            "\n",
            "    File \"/tmp/ipython-input-2848626690.py\", line 43, in preprocess  *\n",
            "        image = tf.cast(image / 255.0, tf.float32) # Normalize pixel values to [0, 1]\n",
            "\n",
            "    TypeError: `x` and `y` must have the same dtype, got tf.uint8 != tf.float32.\n",
            "\n",
            "Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\n",
            "Example: '/content/drive/MyDrive/DSA_Project_Folder'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5df00879"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `tf.cast` operation was applied after the division, leading to a `TypeError` due to implicit type conversion during division. The correct approach is to first cast the image to `tf.float32` and then perform the division for normalization. This ensures explicit type handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "531b65ec",
        "outputId": "db2f5112-3e2e-43e4-c906-58cbba188f51"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Set the dataset_root variable to the confirmed path\n",
        "dataset_root = '/content/drive/MyDrive/aa/dataset'\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    # 3. Load the image dataset using image_dataset_from_directory\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # 4. Print class names and approximate number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    # Corrected: Handle cardinality carefully\n",
        "    total_batches_cardinality = tf.data.experimental.cardinality(raw_train_ds)\n",
        "    if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "        print(\"Total number of batches could not be determined (tf.data.AUTOTUNE). Dataset size might be very large.\")\n",
        "    else:\n",
        "        # Ensure we call .numpy() only if it's a TensorFlow Tensor\n",
        "        total_batches = total_batches_cardinality.numpy() if hasattr(total_batches_cardinality, 'numpy') else total_batches_cardinality\n",
        "        total_images = total_batches * batch_size\n",
        "        print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # 5. Preprocessing function for normalization (corrected)\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image, tf.float32) / 255.0 # Cast image to float32 first, then divide\n",
        "        return image, label\n",
        "\n",
        "    # 6. Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # 7. Verify preprocessing by taking one batch\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"\\nShape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: /content/drive/MyDrive/aa/dataset\n",
            "Found 10634 files belonging to 9 classes.\n",
            "\n",
            "Classes found: ['10', '100', '1000', '2', '20', '200', '5', '50', '500']\n",
            "Number of classes: 9\n",
            "Approximate total number of images in the dataset: 10656\n",
            "\n",
            "Image normalization preprocessing applied to the dataset.\n",
            "\n",
            "Shape of one image batch after preprocessing: (32, 128, 128, 3)\n",
            "Min pixel value in one image batch: 0.0\n",
            "Max pixel value in one image batch: 1.0\n",
            "Shape of one label batch: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26b2dc55"
      },
      "source": [
        "## Split Dataset\n",
        "\n",
        "### Subtask:\n",
        "Split the prepared dataset into training, validation, and test sets to properly evaluate the model's generalization capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8457cb76"
      },
      "source": [
        "### Step 4: Split the Dataset into Training, Validation, and Test Sets\n",
        "\n",
        "Now that the dataset is loaded and preprocessed, the next crucial step is to split it into training, validation, and test sets. This ensures that the model is trained on one part of the data, fine-tuned on another (validation), and finally evaluated on unseen data (test set) to provide an unbiased assessment of its performance.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Determine Total Images**: We'll use the `total_images` variable calculated in the previous step as an approximation for the dataset size.\n",
        "2.  **Define Split Ratios**: We will use a standard split of 70% for training, 15% for validation, and 15% for testing.\n",
        "3.  **Calculate Batches**: The dataset is already batched, so we will determine the number of batches for each split based on the ratios and the overall number of batches.\n",
        "4.  **Create Subsets**: TensorFlow's `tf.data.Dataset` API provides efficient methods (`take`, `skip`) for creating these subsets.\n",
        "5.  **Optimize Performance**: Apply `.cache()` and `.prefetch()` to each dataset split to improve training efficiency. `.cache()` keeps images in memory after the first epoch, and `.prefetch()` overlaps data preprocessing and model execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8b2691"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide the Python code to split the `processed_train_ds` into training, validation, and test sets according to the specified ratios, calculate the number of batches for each split, and apply performance optimizations like `.cache()` and `.prefetch()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af011d0f",
        "outputId": "f8bf22d7-f83c-4671-caa5-950eccd1ce3f"
      },
      "source": [
        "import math\n",
        "\n",
        "# Assuming `processed_train_ds` and `batch_size` are available from previous steps\n",
        "# `total_images` was also calculated, but it's more robust to calculate based on `processed_train_ds` cardinality if possible.\n",
        "# Let's re-evaluate total batches to be sure, assuming `raw_train_ds` is the source.\n",
        "\n",
        "# Get the total number of batches from the processed dataset\n",
        "total_batches_cardinality = tf.data.experimental.cardinality(processed_train_ds)\n",
        "if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "    # Fallback to the approximate total_images if cardinality is not concrete\n",
        "    if 'total_images' in locals():\n",
        "        total_images_approx = total_images # Use previously calculated approximate total_images\n",
        "        total_batches_approx = math.ceil(total_images_approx / batch_size)\n",
        "    else:\n",
        "        print(\"Warning: Cannot determine total number of batches or images for splitting. Please ensure 'processed_train_ds' is properly formed.\")\n",
        "        # For demonstration, assign a default value or raise an error\n",
        "        total_batches_approx = 100 # Placeholder for a real scenario, this would cause issues\n",
        "else:\n",
        "    total_batches_approx = total_batches_cardinality.numpy() # Convert Tensor to Python int\n",
        "\n",
        "print(f\"Total batches in processed dataset: {total_batches_approx}\")\n",
        "\n",
        "# Define split ratios\n",
        "train_split_ratio = 0.7\n",
        "val_split_ratio = 0.15\n",
        "test_split_ratio = 0.15\n",
        "\n",
        "# Calculate number of batches for each split\n",
        "train_batches = int(total_batches_approx * train_split_ratio)\n",
        "val_batches = int(total_batches_approx * val_split_ratio)\n",
        "\n",
        "# The remaining batches go to the test set\n",
        "test_batches = total_batches_approx - train_batches - val_batches\n",
        "\n",
        "print(f\"\\nSplitting dataset:\")\n",
        "print(f\"  Training batches: {train_batches}\")\n",
        "print(f\"  Validation batches: {val_batches}\")\n",
        "print(f\"  Test batches: {test_batches}\")\n",
        "\n",
        "# Create the training, validation, and test datasets\n",
        "train_ds = processed_train_ds.take(train_batches)\n",
        "val_ds = processed_train_ds.skip(train_batches).take(val_batches)\n",
        "test_ds = processed_train_ds.skip(train_batches).skip(val_batches)\n",
        "\n",
        "# Verify the number of batches in each split (optional, for debugging)\n",
        "print(f\"\\nBatches after splitting:\")\n",
        "print(f\"  Train dataset batches (actual): {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"  Validation dataset batches (actual): {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(f\"  Test dataset batches (actual): {tf.data.experimental.cardinality(test_ds).numpy()}\")\n",
        "\n",
        "# Optimize performance with .cache() and .prefetch()\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"\\nDatasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches in processed dataset: 333\n",
            "\n",
            "Splitting dataset:\n",
            "  Training batches: 233\n",
            "  Validation batches: 49\n",
            "  Test batches: 51\n",
            "\n",
            "Batches after splitting:\n",
            "  Train dataset batches (actual): 233\n",
            "  Validation dataset batches (actual): 49\n",
            "  Test dataset batches (actual): 51\n",
            "\n",
            "Datasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759bbd0c"
      },
      "source": [
        "## Build and Compile CNN Model\n",
        "\n",
        "### Subtask:\n",
        "Build a Convolutional Neural Network (CNN) model suitable for image classification, including custom classification layers, and compile it with an appropriate optimizer, loss function, and metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba43a14"
      },
      "source": [
        "### Step 5: Build and Compile the CNN Model\n",
        "\n",
        "Now we will construct the Convolutional Neural Network (CNN) model. We'll leverage transfer learning by using a pre-trained base model (MobileNetV2) and then add custom classification layers on top. Finally, we'll compile the model with suitable settings for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87345084"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the detailed instructions, I will generate Python code to build and compile the CNN model using a pre-trained MobileNetV2 base, add custom classification layers, and then compile it with the specified optimizer, loss function, and metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "fec13c3b",
        "outputId": "5ec4c4db-f5be-449a-bdca-4213018a3428"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "# Assuming img_height, img_width, and class_names are defined from previous steps\n",
        "# img_height = 128\n",
        "# img_width = 128\n",
        "# class_names = ['10', '100', '1000', '2', '20', '200', '5', '50', '500'] # Example\n",
        "\n",
        "print(f\"Image dimensions: ({img_height}, {img_width})\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# 1. Load a pre-trained base model (MobileNetV2) without its top classification layer\n",
        "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
        "                           include_top=False,\n",
        "                           weights='imagenet')\n",
        "\n",
        "# 2. Freeze the base model's layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create a new Sequential model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(), # Converts feature maps to a single vector\n",
        "    Dense(128, activation='relu'), # A hidden dense layer\n",
        "    Dense(len(class_names), activation='softmax') # Output layer for multi-class classification\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Print a summary of the model\n",
        "print(\"\\nModel Architecture Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCNN model built and compiled successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dimensions: (128, 128)\n",
            "Number of classes: 9\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Model Architecture Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,423,113\u001b[0m (9.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,423,113</span> (9.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m165,129\u001b[0m (645.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">165,129</span> (645.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN model built and compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75561cfa"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled CNN model using the training dataset and validate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f4f4a85"
      },
      "source": [
        "### Step 6: Train the CNN Model\n",
        "\n",
        "Now, we will train the compiled CNN model using the prepared training and validation datasets. The training process will iterate over the `train_ds` for a specified number of epochs, and evaluate its performance on the `val_ds` after each epoch. The training history, including loss and accuracy for both training and validation, will be captured for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063cc75a"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I will now generate the Python code to train the compiled CNN model using the `train_ds` and `val_ds`, store the training history, and print a completion message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "dbd0de99",
        "outputId": "04cb6fbd-528d-4beb-d278-fb19b1b3427a"
      },
      "source": [
        "epochs = 10 # Define the number of training epochs\n",
        "\n",
        "print(f\"\\nStarting model training for {epochs} epochs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"\\nModel training completed.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training for 10 epochs...\n",
            "Epoch 1/10\n",
            "\u001b[1m 43/233\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:45\u001b[0m 5s/step - accuracy: 0.7691 - loss: 0.8415"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3104768137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nStarting model training for {epochs} epochs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de26ffa8"
      },
      "source": [
        "# Task\n",
        "Build and evaluate a Convolutional Neural Network (CNN) model to classify banknote images, using the dataset from the Google Drive folder located at 'https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA'. The task involves accessing the data, preparing it, splitting it into training, validation, and test sets, building and training the CNN model, evaluating its performance with relevant metrics, and providing a summary of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05d5175"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled CNN model using the training dataset and validate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a92f7f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training run was interrupted, so I will provide the code to re-run the model training with the specified datasets and store the training history, adhering to the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49525ec9",
        "outputId": "7cb62201-64fb-4a79-9d43-6f34a9bacc88"
      },
      "source": [
        "epochs = 10 # Define the number of training epochs\n",
        "\n",
        "print(f\"\\nStarting model training for {epochs} epochs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"\\nModel training completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training for 10 epochs...\n",
            "Epoch 1/10\n",
            "\u001b[1m124/233\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5:49\u001b[0m 3s/step - accuracy: 0.9888 - loss: 0.0472"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/resnetv2_trained_model.keras')"
      ],
      "metadata": {
        "id": "KoGUmD1F7yrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}