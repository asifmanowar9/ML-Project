{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyMwDNgowOJuVhZ7aaR7PO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifmanowar9/ML-Project/blob/main/resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqlQDaFYzVIG",
        "outputId": "36d12ccd-ad30-4879-a753-425021e91bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chNsnNxAz8jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3094fa4e"
      },
      "source": [
        "### Load the Dataset\n",
        "\n",
        "First, let's load your dataset using `tf.keras.utils.image_dataset_from_directory`. This function is very convenient for datasets organized into subdirectories per class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a17f9386",
        "outputId": "60a3364f-8078-41ba-fc00-427fcb224530"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the base path to your dataset\n",
        "dataset_base_path = '/content/drive/MyDrive/Datasets/ML Datasets/dataset'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_HEIGHT = 224 # Common size for many pre-trained models\n",
        "IMG_WIDTH = 224  # Common size for many pre-trained models\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load the training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_base_path + '/train',\n",
        "    labels='inferred',\n",
        "    label_mode='int', # Return integer encoded labels (0, 1, 2, ...)\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load the validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_base_path + '/val',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # Typically no need to shuffle validation data\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_base_path + '/test',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # Typically no need to shuffle test data\n",
        ")\n",
        "\n",
        "print(\"Datasets loaded successfully!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10093 files belonging to 4 classes.\n",
            "Found 2161 files belonging to 4 classes.\n",
            "Found 2167 files belonging to 4 classes.\n",
            "Datasets loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54e43813"
      },
      "source": [
        "### Inspect the Dataset\n",
        "\n",
        "Let's check the class names and the shape of a batch of images and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cf78c1a",
        "outputId": "a1d78427-fa38-4e62-daa6-c400c4fa8b21"
      },
      "source": [
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "for image_batch, labels_batch in train_ds.take(1):\n",
        "    print(f\"Image batch shape: {image_batch.shape}\") # (batch_size, img_height, img_width, channels)\n",
        "    print(f\"Labels batch shape: {labels_batch.shape}\") # (batch_size,)\n",
        "    print(f\"Sample labels: {labels_batch.numpy()}\")\n",
        "\n",
        "# You can also manually verify the number of samples in each dataset (optional)\n",
        "# num_train_samples = tf.data.experimental.cardinality(train_ds).numpy() * BATCH_SIZE\n",
        "# num_val_samples = tf.data.experimental.cardinality(val_ds).numpy() * BATCH_SIZE\n",
        "# num_test_samples = tf.data.experimental.cardinality(test_ds).numpy() * BATCH_SIZE\n",
        "# print(f\"Number of training samples (approx): {num_train_samples}\")\n",
        "# print(f\"Number of validation samples (approx): {num_val_samples}\")\n",
        "# print(f\"Number of test samples (approx): {num_test_samples}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class names: ['100', '1000', '50', '500']\n",
            "Number of classes: 4\n",
            "Image batch shape: (32, 224, 224, 3)\n",
            "Labels batch shape: (32,)\n",
            "Sample labels: [3 1 1 1 2 1 2 2 3 1 0 2 3 1 1 3 3 3 0 3 3 1 3 0 0 1 3 1 0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ddb099"
      },
      "source": [
        "### Model Choice for Beginners (Image Classification)\n",
        "\n",
        "For image classification, especially as a beginner, I highly recommend starting with **Transfer Learning** using a pre-trained Convolutional Neural Network (CNN).\n",
        "\n",
        "**Why Transfer Learning?**\n",
        "\n",
        "1.  **Less Data Needed**: Training a deep CNN from scratch requires a very large dataset. With transfer learning, you can achieve good results even with a relatively smaller dataset, as the pre-trained model has already learned powerful features from a massive dataset (like ImageNet).\n",
        "2.  **Faster Training**: You don't need to train the entire model from scratch, significantly reducing training time and computational resources.\n",
        "3.  **Good Performance**: Pre-trained models often serve as excellent feature extractors, providing a strong baseline for performance.\n",
        "\n",
        "**Recommended Pre-trained Models to Start With:**\n",
        "\n",
        "*   **MobileNetV2**: This is a lightweight and efficient model, great for beginners as it's faster to train and uses less memory. It's a good balance of accuracy and computational cost.\n",
        "*   **ResNet50**: A more powerful model than MobileNetV2, offering higher accuracy but requiring more computational resources. It's a classic choice for many image classification tasks.\n",
        "*   **VGG16/VGG19**: These are older but still effective models. They are conceptually simpler to understand but are generally larger and slower than ResNet or MobileNet.\n",
        "\n",
        "**How to use them (High-Level Steps):**\n",
        "\n",
        "1.  **Load the pre-trained model**: You'll usually load its convolutional base (without the top classification layers).\n",
        "2.  **Freeze the base layers**: Prevent the pre-trained layers from being updated during training. This keeps the learned features intact.\n",
        "3.  **Add your own classification head**: Attach a few new layers (e.g., `Flatten`, `Dense` layers with an appropriate activation function like `softmax` for multi-class classification) on top of the frozen base.\n",
        "4.  **Train the new layers**: Only train the newly added classification head on your specific dataset.\n",
        "5.  **(Optional) Fine-tuning**: Unfreeze some of the top layers of the pre-trained base and train them along with your classification head using a very small learning rate. This allows the model to adapt its features slightly to your specific task.\n",
        "\n",
        "I recommend starting with **MobileNetV2** as it's a great entry point for transfer learning. Once you're comfortable with that, you can experiment with ResNet50."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PfTW1rmX3X7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627d9187"
      },
      "source": [
        "# Task\n",
        "Implement and train a ResNet50 model for classifying Bangladeshi bank notes from the image dataset located at `/content/drive/MyDrive/Datasets/ML Datasets/dataset`. The task involves preprocessing the datasets, loading a pre-trained ResNet50 base model with frozen layers, building a new classification head, combining them, compiling the model with appropriate settings, training it, and finally evaluating its performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c75e4ca2"
      },
      "source": [
        "## Prepare Datasets for Training\n",
        "\n",
        "### Subtask:\n",
        "Apply image normalization (rescaling pixel values to [0, 1]) to the datasets and configure them for optimal performance using caching and prefetching.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941c2029"
      },
      "source": [
        "## Prepare Datasets for Training\n",
        "\n",
        "### Subtask:\n",
        "Apply image normalization (rescaling pixel values to [0, 1]) to the datasets and configure them for optimal performance using caching and prefetching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f532a5cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the instructions by defining the rescaling layer, applying it to the datasets, caching them, and then applying prefetching for optimal performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4e683d3",
        "outputId": "1ea581b6-7bed-42bb-9f23-30b0891ce8af"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define a tf.keras.layers.Rescaling layer to normalize pixel values from [0, 255] to [0, 1]\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "# 2. Apply this normalization layer to the train_ds, val_ds, and test_ds datasets using the .map() method.\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# 3. Cache each of the normalized datasets using the .cache() method.\n",
        "train_ds = train_ds.cache()\n",
        "val_ds = val_ds.cache()\n",
        "test_ds = test_ds.cache()\n",
        "\n",
        "# 4. Apply prefetching to each cached dataset using the .prefetch() method with tf.data.AUTOTUNE.\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Datasets normalized, cached, and prefetched successfully!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets normalized, cached, and prefetched successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dc91587"
      },
      "source": [
        "## Load Pre-trained ResNet50 Base Model\n",
        "\n",
        "### Subtask:\n",
        "Load the ResNet50 model from `tf.keras.applications` with `weights='imagenet'` and `include_top=False`, which provides the convolutional base without the final classification layers. This base will serve as a feature extractor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fc07f44"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the ResNet50 model, initializing it with specific parameters, and freezing its layers. This code block will perform all these actions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6c753bf",
        "outputId": "e2046dc3-da28-4bf6-a6f3-24ef0d584c5b"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# 1. Load the ResNet50 model without the top classification layer\n",
        "resnet_base = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
        ")\n",
        "\n",
        "# 2. Freeze the layers of the base model\n",
        "resnet_base.trainable = False\n",
        "\n",
        "print(\"ResNet50 base model loaded and layers frozen successfully!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "ResNet50 base model loaded and layers frozen successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fa31d2"
      },
      "source": [
        "## Build the Classification Head\n",
        "\n",
        "### Subtask:\n",
        "Create a new classification head consisting of layers like GlobalAveragePooling2D and Dense layers, with the final Dense layer having num_classes outputs and a softmax activation function for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a521329"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to build a classification head for the ResNet50 model. This involves defining a sequential model with GlobalAveragePooling2D, a Dense layer with ReLU activation, and a final Dense layer with softmax activation based on the number of classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "2420977e",
        "outputId": "83e04145-9fa5-4f6b-f0f3-cb0251bf6876"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Get the number of classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 2. Create a tf.keras.Sequential model for the classification head\n",
        "classification_head = tf.keras.Sequential([\n",
        "    # 3. Add a GlobalAveragePooling2D() layer\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    # 4. Add a Dense layer with ReLU activation\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # 5. Add a final Dense layer with num_classes units and softmax activation\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "print(f\"Classification head created with {num_classes} output classes.\")\n",
        "classification_head.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification head created with 4 output classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ global_average_pooling2d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ global_average_pooling2d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "764c1f99"
      },
      "source": [
        "## Combine Base Model and Classification Head\n",
        "\n",
        "### Subtask:\n",
        "Combine the frozen ResNet50 base model (`resnet_base`) with the custom classification head (`classification_head`) using `tf.keras.Sequential` to create the final model for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ef2e538"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the frozen ResNet50 base model with the custom classification head to create the final model for training. This involves stacking them sequentially.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "d9d5b294",
        "outputId": "ec605125-34d6-4d8e-fc10-1c7f0ef8e9e2"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    resnet_base,\n",
        "    classification_head\n",
        "])\n",
        "\n",
        "print(\"Combined model created successfully!\")\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined model created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │       \u001b[38;5;34m262,788\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,850,500\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,850,500</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,788\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d59aa4e8"
      },
      "source": [
        "## Compile the Model\n",
        "\n",
        "### Subtask:\n",
        "Compile the combined `model` using an appropriate optimizer, loss function, and metrics. For multi-class classification, `Adam` optimizer, `SparseCategoricalCrossentropy` loss (since labels are integer-encoded), and `accuracy` metric are common choices.\n",
        "\n",
        "#### Instructions:\n",
        "1.  **Select Optimizer**: Use `tf.keras.optimizers.Adam`.\n",
        "2.  **Select Loss Function**: Use `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)` because the model's final layer uses `softmax` activation, which outputs probabilities (not logits).\n",
        "3.  **Select Metrics**: Monitor `accuracy` during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ae73c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires compiling the combined model with the specified optimizer, loss function, and metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93755f14",
        "outputId": "4436d7f4-f059-471b-96b3-b501566b27d3"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aaef86c"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled `model` using the `train_ds` for training and `val_ds` for validation. Monitor the `accuracy` and `loss` during training.\n",
        "\n",
        "#### Instructions:\n",
        "1.  **Set Epochs**: Choose an appropriate number of training epochs (e.g., 10-20 is a good starting point for transfer learning).\n",
        "2.  **Train the model**: Use `model.fit()` with `train_ds` and `val_ds`.\n",
        "3.  **Store History**: Store the training history in a variable (e.g., `history`) to visualize training progress later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbc9520e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the model is compiled, the next logical step is to train it using the prepared `train_ds` and `val_ds` datasets. This involves specifying the number of epochs and capturing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7501002",
        "outputId": "18d042f0-0ec8-47eb-d6c9-7438992ad7d7"
      },
      "source": [
        "EPOCHS = 10 # You can adjust this number\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"Model training complete. History stored.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2135s\u001b[0m 7s/step - accuracy: 0.3026 - loss: 1.3912 - val_accuracy: 0.4063 - val_loss: 1.2781\n",
            "Epoch 2/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1995s\u001b[0m 6s/step - accuracy: 0.4037 - loss: 1.2786 - val_accuracy: 0.4540 - val_loss: 1.2241\n",
            "Epoch 3/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2004s\u001b[0m 6s/step - accuracy: 0.4340 - loss: 1.2364 - val_accuracy: 0.4729 - val_loss: 1.1918\n",
            "Epoch 4/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1994s\u001b[0m 6s/step - accuracy: 0.4547 - loss: 1.2073 - val_accuracy: 0.4826 - val_loss: 1.1616\n",
            "Epoch 5/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1988s\u001b[0m 6s/step - accuracy: 0.4734 - loss: 1.1803 - val_accuracy: 0.4998 - val_loss: 1.1336\n",
            "Epoch 6/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2036s\u001b[0m 6s/step - accuracy: 0.4910 - loss: 1.1509 - val_accuracy: 0.5095 - val_loss: 1.1117\n",
            "Epoch 7/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2044s\u001b[0m 6s/step - accuracy: 0.5062 - loss: 1.1275 - val_accuracy: 0.5211 - val_loss: 1.0929\n",
            "Epoch 8/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2011s\u001b[0m 6s/step - accuracy: 0.5163 - loss: 1.1044 - val_accuracy: 0.5243 - val_loss: 1.0841\n",
            "Epoch 9/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2008s\u001b[0m 6s/step - accuracy: 0.5273 - loss: 1.0853 - val_accuracy: 0.5382 - val_loss: 1.0676\n",
            "Epoch 10/10\n",
            "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2052s\u001b[0m 6s/step - accuracy: 0.5358 - loss: 1.0662 - val_accuracy: 0.5451 - val_loss: 1.0570\n",
            "Model training complete. History stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1AIjeBSQMFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/resnet50_trained_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "huWGcpTnQLv1",
        "outputId": "0d340b3e-ec7c-43d9-bd84-4324dc56289a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2871127588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/resnet50_trained_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9VREA6aOrklF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "3d0e5b43",
        "outputId": "fffffa92-84ca-459b-d3a5-888e3eb4238b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = '/content/drive/MyDrive/resnet50_trained_model.keras'\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(f\"Model loaded successfully from {model_path}!\")\n",
        "loaded_model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/drive/MyDrive/resnet50_trained_model.keras!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │       \u001b[38;5;34m262,788\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,376,078\u001b[0m (92.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,376,078</span> (92.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,788\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m525,578\u001b[0m (2.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">525,578</span> (2.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22f440dd"
      },
      "source": [
        "The model has been loaded and its summary is printed above. You can now use `loaded_model` for further evaluation or predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model on the Test Set\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the `loaded_model` on the `test_ds` to assess its performance on unseen data. This will provide metrics such as loss and accuracy.\n",
        "\n",
        "#### Instructions:\n",
        "1.  Use the `loaded_model.evaluate()` method with the `test_ds`.\n",
        "2.  Print the evaluation results, specifically the test loss and test accuracy."
      ],
      "metadata": {
        "id": "feDeDeMzs-fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":**Reasoning**:\n",
        "Evaluating the model on the test set is crucial to understand its generalization capability. This code will perform the evaluation and print the results."
      ],
      "metadata": {
        "id": "q9C9mV8AvZq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = loaded_model.evaluate(test_ds)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOJt63C0s-LF",
        "outputId": "24437359-5d16-4e02-d2ac-e2711ea8f141"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 10s/step - accuracy: 0.1202 - loss: 101.7305\n",
            "\n",
            "Test Loss: 68.5307\n",
            "Test Accuracy: 0.2423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqYybdYTwfhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00db6bd3"
      },
      "source": [
        "# Task\n",
        "Prepare a sample image from `test_ds` for prediction, ensuring it is correctly shaped and normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75b13164"
      },
      "source": [
        "## Prepare a Sample Image for Prediction\n",
        "\n",
        "### Subtask:\n",
        "Select a single image from the `test_ds` dataset and prepare it for prediction. This involves extracting the image, ensuring it has the correct shape (adding a batch dimension), and confirming it's normalized, similar to how training data was processed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d5e28d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires extracting a single image from the `test_ds` and preparing it for prediction by adding a batch dimension. This code block will iterate through the test dataset, take one batch, extract the first image and its label, and then expand the image's dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41c703d8",
        "outputId": "66942ab2-999e-4e9c-d70d-720c2841cc5d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get a single batch from the test dataset\n",
        "for image_batch, label_batch in test_ds.take(1):\n",
        "    # Extract the first image and its label\n",
        "    sample_image = image_batch[0]\n",
        "    sample_label = label_batch[0]\n",
        "\n",
        "# Add a batch dimension to the sample image\n",
        "sample_image_expanded = tf.expand_dims(sample_image, 0)\n",
        "\n",
        "print(f\"Original image shape: {sample_image.shape}\")\n",
        "print(f\"Expanded image shape (ready for prediction): {sample_image_expanded.shape}\")\n",
        "print(f\"Sample label (raw integer): {sample_label.numpy()}\")\n",
        "print(f\"Sample label (class name): {class_names[sample_label.numpy()]}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image shape: (224, 224, 3)\n",
            "Expanded image shape (ready for prediction): (1, 224, 224, 3)\n",
            "Sample label (raw integer): 0\n",
            "Sample label (class name): 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba2fdec"
      },
      "source": [
        "# Task\n",
        "Interpret the model's probability prediction for the sample image. Identify the class with the highest probability and map it to its human-readable class name using the `class_names` list. Then, present the predicted class label, its confidence score, and compare it with the actual label of the sample image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820da21e"
      },
      "source": [
        "## Interpret the Prediction\n",
        "\n",
        "### Subtask:\n",
        "Convert the model's probability output into a human-readable class label. This involves finding the class with the highest probability and mapping it back to the original class name using `class_names`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a30a4e8e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires converting the model's raw probability output into a human-readable class label by finding the class with the highest probability and then mapping that index to its corresponding class name. This code will perform the prediction using the loaded model, then extract and print the predicted class and its confidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa593ae3",
        "outputId": "7d7bb1b4-9d50-48bf-f525-309d9bba92ab"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Make a prediction using the loaded model on the expanded sample image\n",
        "predictions = loaded_model.predict(sample_image_expanded)\n",
        "\n",
        "# 1. Find the index of the class with the highest probability\n",
        "predicted_class_index = np.argmax(predictions[0])\n",
        "\n",
        "# 2. Extract the confidence score (the maximum probability)\n",
        "confidence_score = predictions[0][predicted_class_index]\n",
        "\n",
        "# 3. Use the class_names list to get the human-readable predicted class label\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "\n",
        "# 4. Print the predicted class label and its corresponding confidence score\n",
        "print(f\"\\nModel Predictions (probabilities): {predictions[0]}\")\n",
        "print(f\"Predicted Class Index: {predicted_class_index}\")\n",
        "print(f\"Predicted Class: {predicted_class_name} (Confidence: {confidence_score:.4f})\")\n",
        "print(f\"Actual Class: {class_names[sample_label.numpy()]}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\n",
            "Model Predictions (probabilities): [0.020114   0.02140551 0.01921411 0.9392664 ]\n",
            "Predicted Class Index: 3\n",
            "Predicted Class: 500 (Confidence: 0.9393)\n",
            "Actual Class: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fd17747"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the predicted class label for the sample image, along with the actual label if available, and any relevant confidence scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5281538e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Predicted Class Label:** The predicted class label is \"500\".\n",
        "*   **Confidence Score:** The confidence score for the predicted class \"500\" is 0.9393.\n",
        "*   **Actual Label:** The actual class label for the sample image is \"100\".\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The model predicted the class \"500\" with a high confidence score of 0.9393.\n",
        "*   The probabilities for all classes were: \\[0.020114, 0.02140551, 0.01921411, 0.9392664]. The highest probability corresponds to the index 3.\n",
        "*   The actual class of the sample image was \"100\", which differs from the model's prediction.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The model confidently misclassified the sample image, predicting \"500\" instead of the actual \"100\". This suggests a potential area for further model evaluation, especially on specific misclassified examples.\n",
        "*   Investigate the features of the misclassified image (actual class \"100\") and compare them to features typically associated with class \"500\" to understand the source of the model's error.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFxxe_Bl71mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ca46166"
      },
      "source": [
        "# Task\n",
        "Load MobileNetV2 base model from `tf.keras.applications` with `weights='imagenet'` and `include_top=False`, freezing its layers. Then, combine this frozen base with the previously defined classification head using `tf.keras.Sequential`. Compile the new model with `tf.keras.optimizers.Adam`, `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)`, and `accuracy` as a metric. Train the compiled model for 5-10 epochs using `train_ds` and `val_ds`, storing the training history. Finally, summarize the training process, including time taken and MobileNetV2 model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cf0667"
      },
      "source": [
        "## Load MobileNetV2 Base Model\n",
        "\n",
        "### Subtask:\n",
        "Load the MobileNetV2 model from `tf.keras.applications` with `weights='imagenet'` and `include_top=False`, providing the convolutional base without the final classification layers. Freeze its layers to use it as a feature extractor. This replaces the previous ResNet50 base model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af346d42"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the MobileNetV2 model, initializing it with specific parameters, and freezing its layers. This code block will perform all these actions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55e1df44",
        "outputId": "df3ee872-3221-4416-dd58-4578e15f6f16"
      },
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "# 1. Load the MobileNetV2 model without the top classification layer\n",
        "mobilenet_base = MobileNetV2(\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# 2. Freeze the layers of the base model\n",
        "mobilenet_base.trainable = False\n",
        "\n",
        "print(\"MobileNetV2 base model loaded and layers frozen successfully!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "MobileNetV2 base model loaded and layers frozen successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc4f035b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires combining the newly loaded MobileNetV2 base model with the existing classification head to form the complete model architecture. This will be done using `tf.keras.Sequential`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "78c20e38",
        "outputId": "0f109649-8174-43ed-c059-aa3f16a3fba7"
      },
      "source": [
        "model_mobilenet = tf.keras.Sequential([\n",
        "    mobilenet_base,\n",
        "    classification_head\n",
        "])\n",
        "\n",
        "print(\"Combined MobileNetV2 model created successfully!\")\n",
        "model_mobilenet.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 7, 7, 2048), found shape=(None, 7, 7, 1280)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, ragged=False, name=keras_tensor_710>',)\n  • kwargs={'mask': 'None'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-191876852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_mobilenet = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmobilenet_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclassification_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, trainable, name)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_rebuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36m_maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_lock_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mbuild_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0moriginal_build_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;31m# Record build config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_build_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Can happen if shape inference is not implemented.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 7, 7, 2048), found shape=(None, 7, 7, 1280)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, ragged=False, name=keras_tensor_710>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6d4bd95"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error occurred because the `classification_head` object was already 'built' with an input shape compatible with ResNet50's output (2048 features). When attempting to combine it with MobileNetV2's output (1280 features), a shape mismatch occurred. To fix this, I will create a new instance of the classification head, ensuring it is built correctly to match MobileNetV2's output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ed6ab39f",
        "outputId": "9d61bbda-d0f0-4751-db8b-ae789c9bb1de"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Re-create the classification head to ensure it's not pre-built with ResNet50's output shape\n",
        "# This ensures it adapts to MobileNetV2's output\n",
        "new_classification_head_for_mobilenet = tf.keras.Sequential([\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_mobilenet = tf.keras.Sequential([\n",
        "    mobilenet_base,\n",
        "    new_classification_head_for_mobilenet\n",
        "])\n",
        "\n",
        "print(\"Combined MobileNetV2 model created successfully!\")\n",
        "model_mobilenet.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined MobileNetV2 model created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_3 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │       \u001b[38;5;34m164,484\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,484</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,468\u001b[0m (9.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,468</span> (9.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,484\u001b[0m (642.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,484</span> (642.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd5ffe4"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to compile the newly created `model_mobilenet` with the specified optimizer, loss function, and metrics, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33dbd5f1",
        "outputId": "95328f10-51fe-4f45-d4cc-f9d79a49e8af"
      },
      "source": [
        "model_mobilenet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"MobileNetV2 model compiled successfully!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV2 model compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc689027"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the MobileNetV2 model is compiled, the next logical step is to train it using the prepared `train_ds` and `val_ds` datasets. This involves specifying the number of epochs (5-10 as per the task) and capturing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "bf295ffe",
        "outputId": "cc2b0b3e-ff67-403b-bfd3-532658f701cc"
      },
      "source": [
        "EPOCHS_MOBILENET = 10 # Adjust this number as needed, typically 5-10 for transfer learning\n",
        "\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_MOBILENET,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"MobileNetV2 model training complete. History stored.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m  9/316\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38:28\u001b[0m 8s/step - accuracy: 0.4730 - loss: 1.3559"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1351717195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS_MOBILENET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# Adjust this number as needed, typically 5-10 for transfer learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history_mobilenet = model_mobilenet.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS_MOBILENET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "907c9f06"
      },
      "source": [
        "# Task\n",
        "Reduce the size of `train_ds`, `val_ds`, and `test_ds` by applying a `take()` operation to each, and then verify the new number of batches and approximate number of samples for each dataset. Re-compile `model_mobilenet` with `tf.keras.optimizers.Adam`, `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)`, and `accuracy` metric. Finally, train the re-compiled `model_mobilenet` on the reduced `train_ds` and `val_ds` for 5-10 epochs, storing the training history, and then evaluate its performance on the reduced `test_ds`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edc72c23"
      },
      "source": [
        "## Reduce Dataset Size\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing `train_ds`, `val_ds`, and `test_ds` by applying a `take()` operation to significantly reduce the number of samples in each dataset. This will make training much faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad02aca7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the existing datasets by applying a `take()` operation to reduce their size, followed by re-caching and prefetching for optimal performance. This code block will perform all these actions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "effef80b",
        "outputId": "2750a2d9-a66f-49d6-e149-eaa3bec08b74"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define a variable to specify how many batches to take from each dataset\n",
        "reduced_batches_count = 20 # You can adjust this number\n",
        "\n",
        "# 2. Apply the .take() method to reduce the number of samples in each dataset\n",
        "train_ds = train_ds.take(reduced_batches_count)\n",
        "val_ds = val_ds.take(reduced_batches_count)\n",
        "test_ds = test_ds.take(reduced_batches_count)\n",
        "\n",
        "print(f\"Datasets reduced to {reduced_batches_count} batches each.\")\n",
        "\n",
        "# 3. Re-apply caching and prefetching to the new, smaller datasets\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Reduced datasets re-cached and prefetched successfully!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets reduced to 20 batches each.\n",
            "Reduced datasets re-cached and prefetched successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce420fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the datasets have been reduced, it's important to verify their new sizes by checking the number of batches and approximate number of samples, which is part of the overall task instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14166eda",
        "outputId": "123e2d04-0f87-41db-d219-93e2f9b837e4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Verify the new number of batches in each dataset\n",
        "print(f\"Number of batches in reduced train_ds: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Number of batches in reduced val_ds: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(f\"Number of batches in reduced test_ds: {tf.data.experimental.cardinality(test_ds).numpy()}\")\n",
        "\n",
        "# Calculate and print the approximate number of samples in each reduced dataset\n",
        "# Note: tf.data.experimental.cardinality() returns the number of batches, not samples\n",
        "# Multiply by BATCH_SIZE to get an approximate number of samples\n",
        "\n",
        "# For train_ds, if cardinality is -2, it means the size is unknown, so we use the reduced_batches_count * BATCH_SIZE\n",
        "num_train_samples_approx = tf.data.experimental.cardinality(train_ds).numpy() * BATCH_SIZE if tf.data.experimental.cardinality(train_ds).numpy() != tf.data.INFINITE_CARDINALITY else reduced_batches_count * BATCH_SIZE\n",
        "num_val_samples_approx = tf.data.experimental.cardinality(val_ds).numpy() * BATCH_SIZE if tf.data.experimental.cardinality(val_ds).numpy() != tf.data.INFINITE_CARDINALITY else reduced_batches_count * BATCH_SIZE\n",
        "num_test_samples_approx = tf.data.experimental.cardinality(test_ds).numpy() * BATCH_SIZE if tf.data.experimental.cardinality(test_ds).numpy() != tf.data.INFINITE_CARDINALITY else reduced_batches_count * BATCH_SIZE\n",
        "\n",
        "print(f\"Approximate number of training samples (reduced): {num_train_samples_approx}\")\n",
        "print(f\"Approximate number of validation samples (reduced): {num_val_samples_approx}\")\n",
        "print(f\"Approximate number of test samples (reduced): {num_test_samples_approx}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in reduced train_ds: 20\n",
            "Number of batches in reduced val_ds: 20\n",
            "Number of batches in reduced test_ds: 20\n",
            "Approximate number of training samples (reduced): 640\n",
            "Approximate number of validation samples (reduced): 640\n",
            "Approximate number of test samples (reduced): 640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f8fc9f"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to re-compile the `model_mobilenet` as instructed, using the specified optimizer, loss function, and metrics. This ensures the model is ready for training on the reduced datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22769107",
        "outputId": "c6f9ca05-08cf-457e-d2b9-7a899ddec417"
      },
      "source": [
        "model_mobilenet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"MobileNetV2 model re-compiled successfully!\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV2 model re-compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8f383e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the MobileNetV2 model is re-compiled and the datasets are reduced, the next logical step is to train it using the prepared `train_ds` and `val_ds` datasets. This involves specifying the number of epochs (5-10 as per the task) and capturing the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b2e4de0",
        "outputId": "f2f565e2-195d-4a53-d0d4-fa978eed8c14"
      },
      "source": [
        "EPOCHS_MOBILENET_REDUCED = 10 # Adjust this number as needed, typically 5-10 for transfer learning on reduced dataset\n",
        "\n",
        "history_mobilenet_reduced = model_mobilenet.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_MOBILENET_REDUCED,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"MobileNetV2 model training on reduced datasets complete. History stored.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 15s/step - accuracy: 0.7118 - loss: 0.7404 - val_accuracy: 0.8094 - val_loss: 0.4948\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 4s/step - accuracy: 0.8957 - loss: 0.3301 - val_accuracy: 0.8734 - val_loss: 0.3631\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.9700 - loss: 0.1499 - val_accuracy: 0.9078 - val_loss: 0.2761\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.9834 - loss: 0.1033 - val_accuracy: 0.8906 - val_loss: 0.3038\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3s/step - accuracy: 0.9967 - loss: 0.0758 - val_accuracy: 0.8938 - val_loss: 0.2730\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0548 - val_accuracy: 0.9266 - val_loss: 0.2177\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0489 - val_accuracy: 0.9453 - val_loss: 0.1516\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0398 - val_accuracy: 0.9500 - val_loss: 0.1232\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0309 - val_accuracy: 0.9422 - val_loss: 0.1328\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.9973 - loss: 0.0297 - val_accuracy: 0.9422 - val_loss: 0.1476\n",
            "MobileNetV2 model training on reduced datasets complete. History stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "301818a6"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the MobileNetV2 model has been trained on the reduced dataset, the next logical step is to evaluate its performance on the `test_ds`, as specified in the task instructions. This will provide insights into its generalization capabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "329527aa",
        "outputId": "1f05bb84-6e3a-4b65-f6b1-b48bd45ee714"
      },
      "source": [
        "loss_mobilenet, accuracy_mobilenet = model_mobilenet.evaluate(test_ds)\n",
        "\n",
        "print(f\"\\nMobileNetV2 Test Loss (reduced dataset): {loss_mobilenet:.4f}\")\n",
        "print(f\"MobileNetV2 Test Accuracy (reduced dataset): {accuracy_mobilenet:.4f}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.9564 - loss: 0.1282\n",
            "\n",
            "MobileNetV2 Test Loss (reduced dataset): 0.1867\n",
            "MobileNetV2 Test Accuracy (reduced dataset): 0.9281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/mobilenetv2_reduced_trained_model.keras')"
      ],
      "metadata": {
        "id": "HmJq0YmHBaOu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109e9349"
      },
      "source": [
        "## MobileNetV2 Training and Evaluation Summary (Reduced Dataset)\n",
        "\n",
        "### Training Process:\n",
        "*   **Model**: MobileNetV2 with a custom classification head.\n",
        "*   **Epochs**: 10 (as defined by `EPOCHS_MOBILENET_REDUCED`).\n",
        "*   **Training Dataset**: `train_ds` reduced to 20 batches (approx. 640 samples).\n",
        "*   **Validation Dataset**: `val_ds` reduced to 20 batches (approx. 640 samples).\n",
        "*   **Optimizer**: Adam.\n",
        "*   **Loss Function**: SparseCategoricalCrossentropy.\n",
        "*   **Metrics**: Accuracy.\n",
        "\n",
        "### Training Performance Highlights (from `history_mobilenet_reduced`):\n",
        "*   **Epoch 1**: Train Accuracy: 0.7118, Train Loss: 0.7404 | Val Accuracy: 0.8094, Val Loss: 0.4948\n",
        "*   **Epoch 10**: Train Accuracy: 0.9973, Train Loss: 0.0297 | Val Accuracy: 0.9422, Val Loss: 0.1476\n",
        "\n",
        "### Overall Training Insight:\n",
        "The model trained very quickly on the reduced dataset (each epoch took roughly 1 minute on average, compared to several minutes per epoch for the full dataset). The training accuracy rapidly approached 1.0, and validation accuracy also showed significant improvement, reaching over 94% by the final epoch. This indicates that the MobileNetV2 base, even with a small custom head, is highly effective for transfer learning on this dataset, even with limited samples.\n",
        "\n",
        "### Evaluation on Reduced Test Set:\n",
        "*   **Test Loss**: 0.1867\n",
        "*   **Test Accuracy**: 0.9281\n",
        "\n",
        "### Conclusion:\n",
        "The MobileNetV2 model demonstrated strong performance on the reduced test dataset, achieving an accuracy of approximately 92.81%. This confirms its suitability for the task, especially when computational resources or training time are limited. The rapid convergence and high accuracy on the validation and test sets highlight the efficiency of transfer learning with MobileNetV2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38cbfcb"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the results of training with a reduced dataset, comparing training time and performance to previous attempts, and discuss implications for model development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bb727b6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Comparison of training time and performance with a reduced dataset:** Training on the reduced dataset was significantly faster, with each epoch taking approximately 1 minute, compared to several minutes per epoch for the full dataset (though explicit full dataset performance was not provided in this specific analysis). The model achieved a high validation accuracy of 94.22% by the 10th epoch and a test accuracy of 92.81% on the reduced dataset.\n",
        "*   **Implications for model development:** The rapid convergence and strong performance (92.81% test accuracy) on the reduced dataset highlight the efficiency and effectiveness of transfer learning with MobileNetV2. This approach is highly suitable for scenarios where computational resources are limited or faster prototyping and development cycles are desired, as it still yields a high-performing model with fewer samples.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `train_ds`, `val_ds`, and `test_ds` datasets were successfully reduced to 20 batches each using the `.take()` operation. This resulted in approximately 640 samples per dataset (assuming a `BATCH_SIZE` of 32).\n",
        "*   The MobileNetV2 model was re-compiled with `tf.keras.optimizers.Adam()` and `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)`, using `accuracy` as the metric.\n",
        "*   After 10 epochs of training on the reduced datasets:\n",
        "    *   The training accuracy improved from 0.7118 (Epoch 1) to 0.9973 (Epoch 10).\n",
        "    *   The validation accuracy improved from 0.8094 (Epoch 1) to 0.9422 (Epoch 10).\n",
        "    *   Each epoch completed in approximately 1 minute.\n",
        "*   Evaluation on the reduced test dataset yielded a test loss of 0.1867 and a test accuracy of 0.9281.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Transfer learning with MobileNetV2 is highly effective for this classification task, demonstrating strong performance (92.81% test accuracy) even when trained on a significantly reduced dataset.\n",
        "*   The ability to achieve high accuracy with fewer samples and faster training times makes this approach valuable for rapid prototyping and deployment in resource-constrained environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4UTaOj1EBIWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e51b7d6"
      },
      "source": [
        "# Task\n",
        "Reload the ResNet50 model from `tf.keras.applications` with `weights='imagenet'` and `include_top=False`, ensuring its layers are frozen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7e679e9"
      },
      "source": [
        "## Reload ResNet50 Base Model\n",
        "\n",
        "### Subtask:\n",
        "Load the ResNet50 model from `tf.keras.applications` with `weights='imagenet'` and `include_top=False`, ensuring its layers are frozen. This will be a fresh instance to avoid conflicts with previous model definitions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aa66e64"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires reloading the ResNet50 model with frozen layers to ensure a fresh instance for further use. This involves importing the necessary class, initializing the model, and then freezing its weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdad035d",
        "outputId": "62db0619-4e1f-4a06-fd1e-9f879e0f713f"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# 1. Load the ResNet50 model without the top classification layer\n",
        "resnet_base_reloaded = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
        ")\n",
        "\n",
        "# 2. Freeze the layers of the reloaded base model\n",
        "resnet_base_reloaded.trainable = False\n",
        "\n",
        "print(\"ResNet50 base model reloaded and layers frozen successfully!\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 base model reloaded and layers frozen successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc146a93"
      },
      "source": [
        "## Build a New Classification Head for ResNet50\n",
        "\n",
        "### Subtask:\n",
        "Create a new classification head for the ResNet50 model, consisting of GlobalAveragePooling2D and Dense layers with appropriate activation, ensuring it matches the expected output of the ResNet50 base.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef52f241"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a new classification head for the ResNet50 model, including GlobalAveragePooling2D and Dense layers with appropriate activation. This code block will define and summarize the new classification head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "e873c0bb",
        "outputId": "2b9282f1-13cb-42f5-b714-5155152cbd29"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the number of classes (already defined as num_classes)\n",
        "# num_classes = len(class_names)\n",
        "\n",
        "# Create a new tf.keras.Sequential model for the ResNet50 classification head\n",
        "resnet_classification_head = tf.keras.Sequential([\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "print(f\"New classification head for ResNet50 created with {num_classes} output classes.\")\n",
        "resnet_classification_head.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New classification head for ResNet50 created with 4 output classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ global_average_pooling2d_2      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ global_average_pooling2d_2      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5311f4c0"
      },
      "source": [
        "## Combine ResNet50 Base and New Classification Head\n",
        "\n",
        "### Subtask:\n",
        "Combine the reloaded frozen ResNet50 base model with the newly created classification head using tf.keras.Sequential to form the complete ResNet50 model for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "301d84ab"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the reloaded frozen ResNet50 base model with the new classification head to create the complete ResNet50 model for training, then print its summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "69c82942",
        "outputId": "bfc22a96-c750-4523-aca8-6ff9fdeb32ed"
      },
      "source": [
        "model_resnet = tf.keras.Sequential([\n",
        "    resnet_base_reloaded,\n",
        "    resnet_classification_head\n",
        "])\n",
        "\n",
        "print(\"Combined ResNet50 model created successfully!\")\n",
        "model_resnet.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined ResNet50 model created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_5 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │       \u001b[38;5;34m262,788\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ sequential_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,850,500\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,850,500</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,788\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec135814"
      },
      "source": [
        "## Compile the ResNet50 Model\n",
        "\n",
        "### Subtask:\n",
        "Compile the combined `model_resnet` using an appropriate optimizer, loss function, and metrics. For multi-class classification, `Adam` optimizer, `SparseCategoricalCrossentropy` loss (since labels are integer-encoded), and `accuracy` metric are common choices.\n",
        "\n",
        "#### Instructions:\n",
        "1.  **Select Optimizer**: Use `tf.keras.optimizers.Adam()`.\n",
        "2.  **Select Loss Function**: Use `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)` because the model's final layer uses `softmax` activation, which outputs probabilities (not logits).\n",
        "3.  **Select Metrics**: Monitor `accuracy` during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786c7ead"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully combined the ResNet50 base model with the new classification head. The next logical step is to compile this newly formed model, specifying the optimizer, loss function, and metrics, as part of the overall task workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fc0afbb",
        "outputId": "5dc978eb-da6b-4c86-9942-c98c42d161eb"
      },
      "source": [
        "model_resnet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"ResNet50 model compiled successfully!\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 model compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eafdb62"
      },
      "source": [
        "## Train the ResNet50 Model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled `model_resnet` using the `train_ds` for training and `val_ds` for validation. Monitor the `accuracy` and `loss` during training.\n",
        "\n",
        "#### Instructions:\n",
        "1.  **Set Epochs**: Choose an appropriate number of training epochs (e.g., 5-10 is a good starting point for transfer learning on a reduced dataset).\n",
        "2.  **Train the model**: Use `model_resnet.fit()` with the reduced `train_ds` and `val_ds`.\n",
        "3.  **Store History**: Store the training history in a variable (e.g., `history_resnet`) to compare with previous training runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada4cf54"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the ResNet50 model is compiled, the next logical step is to train it using the prepared (reduced) `train_ds` and `val_ds` datasets, as per the task instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cf7a736",
        "outputId": "8d2d17eb-8f90-4171-9522-930de4895596"
      },
      "source": [
        "EPOCHS_RESNET = 10 # You can adjust this number, typically 5-10 for reduced datasets\n",
        "\n",
        "history_resnet = model_resnet.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_RESNET,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"ResNet50 model training complete. History stored.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 12s/step - accuracy: 0.2791 - loss: 1.4988 - val_accuracy: 0.1109 - val_loss: 1.4103\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 12s/step - accuracy: 0.2696 - loss: 1.4031 - val_accuracy: 0.7828 - val_loss: 1.1002\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 12s/step - accuracy: 0.2978 - loss: 1.3690 - val_accuracy: 0.7141 - val_loss: 1.1480\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 13s/step - accuracy: 0.3408 - loss: 1.3472 - val_accuracy: 0.6859 - val_loss: 1.1678\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 13s/step - accuracy: 0.3432 - loss: 1.3292 - val_accuracy: 0.6391 - val_loss: 1.1926\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 12s/step - accuracy: 0.3656 - loss: 1.3148 - val_accuracy: 0.6219 - val_loss: 1.1950\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 12s/step - accuracy: 0.3689 - loss: 1.3007 - val_accuracy: 0.6062 - val_loss: 1.1899\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 12s/step - accuracy: 0.3721 - loss: 1.2888 - val_accuracy: 0.5922 - val_loss: 1.1859\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 12s/step - accuracy: 0.3913 - loss: 1.2767 - val_accuracy: 0.5656 - val_loss: 1.1839\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 13s/step - accuracy: 0.4147 - loss: 1.2653 - val_accuracy: 0.5375 - val_loss: 1.1844\n",
            "ResNet50 model training complete. History stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/resnet50_reduced_trained_model.keras')"
      ],
      "metadata": {
        "id": "eAVctencNaM_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58739576"
      },
      "source": [
        "## Evaluate the ResNet50 Model on the Test Set\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the `model_resnet` on the reduced `test_ds` to assess its performance on unseen data. This will provide metrics such as loss and accuracy.\n",
        "\n",
        "#### Instructions:\n",
        "1.  Use the `model_resnet.evaluate()` method with the `test_ds`.\n",
        "2.  Print the evaluation results, specifically the test loss and test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efb0bbf"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the ResNet50 model has been trained, the next step is to evaluate its performance on the test set using the `evaluate` method and then print the loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84732ee",
        "outputId": "83e67e57-ecb9-4b23-dcf8-9c572feed534"
      },
      "source": [
        "loss_resnet, accuracy_resnet = model_resnet.evaluate(test_ds)\n",
        "\n",
        "print(f\"\\nResNet50 Test Loss (reduced dataset): {loss_resnet:.4f}\")\n",
        "print(f\"ResNet50 Test Accuracy (reduced dataset): {accuracy_resnet:.4f}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - accuracy: 0.5985 - loss: 1.1744\n",
            "\n",
            "ResNet50 Test Loss (reduced dataset): 1.1941\n",
            "ResNet50 Test Accuracy (reduced dataset): 0.5719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d202b057"
      },
      "source": [
        "## ResNet50 Training and Evaluation Summary (Reduced Dataset)\n",
        "\n",
        "### Training Process:\n",
        "*   **Model**: ResNet50 with a custom classification head.\n",
        "*   **Epochs**: 10 (as defined by `EPOCHS_RESNET`).\n",
        "*   **Training Dataset**: `train_ds` reduced to 20 batches (approx. 640 samples).\n",
        "*   **Validation Dataset**: `val_ds` reduced to 20 batches (approx. 640 samples).\n",
        "*   **Optimizer**: Adam.\n",
        "*   **Loss Function**: SparseCategoricalCrossentropy.\n",
        "*   **Metrics**: Accuracy.\n",
        "\n",
        "### Training Performance Highlights (from `history_resnet`):\n",
        "*   **Epoch 1**: Train Accuracy: 0.2791, Train Loss: 1.4988 | Val Accuracy: 0.1109, Val Loss: 1.4103\n",
        "*   **Epoch 10**: Train Accuracy: 0.4147, Train Loss: 1.2653 | Val Accuracy: 0.5375, Val Loss: 1.1844\n",
        "\n",
        "### Overall Training Insight:\n",
        "Training the ResNet50 model on the reduced dataset took significantly longer per epoch (around 12-13 seconds per step for ~20 steps, totaling around 4-5 minutes per epoch) compared to MobileNetV2 on the reduced dataset (around 3-4 seconds per step). The training and validation accuracy showed improvement over epochs, but the final validation accuracy was 53.75%, which is considerably lower than MobileNetV2's 94.22% on the same reduced dataset. This suggests that while ResNet50 is a powerful model, its performance on this specific reduced dataset with a simple classification head is not as effective as MobileNetV2, perhaps due to its larger parameter count making it harder to fine-tune with very limited data or requiring more elaborate fine-tuning strategies.\n",
        "\n",
        "### Evaluation on Reduced Test Set:\n",
        "*   **Test Loss**: 1.1941\n",
        "*   **Test Accuracy**: 0.5719\n",
        "\n",
        "### Conclusion:\n",
        "The ResNet50 model performed relatively poorly on the reduced test dataset, achieving an accuracy of approximately 57.19%. This is a notable difference compared to MobileNetV2's performance (92.81%) on the same reduced dataset. The choice of base model and dataset size significantly impacts performance and training efficiency, and MobileNetV2 appears to be a more suitable choice for this task under resource-constrained/reduced data conditions due to its efficiency and better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d050897c"
      },
      "source": [
        "## Summary: Comparing MobileNetV2 and ResNet50 on Reduced Dataset\n",
        "\n",
        "### Q&A\n",
        "*   **Comparison of training time and performance with a reduced dataset (MobileNetV2 vs. ResNet50):**\n",
        "    *   **MobileNetV2 (Reduced Dataset):** Training was very fast, with each epoch completing in approximately 1 minute. It achieved a high validation accuracy of 94.22% by the 10th epoch and a test accuracy of 92.81%.\n",
        "    *   **ResNet50 (Reduced Dataset):** Training was significantly slower per epoch, taking around 4-5 minutes per epoch. The model's validation accuracy reached only 53.75% by the 10th epoch, and its test accuracy was 57.19%.\n",
        "    *   **Conclusion:** MobileNetV2 significantly outperformed ResNet50 in both training speed and accuracy on the reduced dataset.\n",
        "\n",
        "*   **Implications for model development:** The rapid convergence and strong performance of MobileNetV2 (92.81% test accuracy) on a significantly reduced dataset highlight its efficiency and effectiveness for transfer learning in resource-constrained environments or when rapid prototyping is needed. In contrast, ResNet50's poorer performance on the reduced dataset suggests that while it is a powerful model, its larger parameter count may make it harder to fine-tune effectively with very limited data, potentially requiring more data, more epochs, or a different fine-tuning strategy (e.g., unfreezing more layers) to achieve comparable results. For this specific task and dataset size, MobileNetV2 is clearly the more suitable and efficient choice.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Both MobileNetV2 and ResNet50 were trained on datasets reduced to 20 batches each (approximately 640 samples per dataset).\n",
        "*   **MobileNetV2 Results:**\n",
        "    *   Training Accuracy: ~0.9973\n",
        "    *   Validation Accuracy: ~0.9422\n",
        "    *   Test Accuracy: 0.9281\n",
        "    *   Epoch Training Time: ~1 minute\n",
        "*   **ResNet50 Results:**\n",
        "    *   Training Accuracy: ~0.4147\n",
        "    *   Validation Accuracy: ~0.5375\n",
        "    *   Test Accuracy: 0.5719\n",
        "    *   Epoch Training Time: ~4-5 minutes\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   MobileNetV2 is highly recommended for this image classification task, especially with limited data or computational resources, due to its superior balance of speed and accuracy compared to ResNet50 under these conditions.\n",
        "*   For ResNet50, further experimentation might include increasing the number of training epochs, implementing learning rate schedules, or exploring fine-tuning by unfreezing some of its top layers, though this would increase training time and require more careful hyperparameter tuning.\n",
        "*   The significantly different performance between the two models emphasizes the importance of selecting an appropriate pre-trained base model based on the dataset size and available resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118b60a3"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the training process and performance of the ResNet50 model on the reduced dataset, comparing it with the MobileNetV2 results and discussing any notable differences or insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f772aff"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Comparison of training time and performance with a reduced dataset (MobileNetV2 vs. ResNet50):**\n",
        "    *   **MobileNetV2 (Reduced Dataset):** Training was very fast, with each epoch completing in approximately 1 minute. It achieved a high validation accuracy of 94.22% by the 10th epoch and a test accuracy of 92.81%.\n",
        "    *   **ResNet50 (Reduced Dataset):** Training was significantly slower per epoch, taking around 4-5 minutes per epoch. The model's validation accuracy reached only 53.75% by the 10th epoch, and its test accuracy was 57.19%.\n",
        "    *   **Conclusion:** MobileNetV2 significantly outperformed ResNet50 in both training speed and accuracy on the reduced dataset.\n",
        "\n",
        "*   **Implications for model development:** The rapid convergence and strong performance of MobileNetV2 (92.81% test accuracy) on a significantly reduced dataset highlight its efficiency and effectiveness for transfer learning in resource-constrained environments or when rapid prototyping is needed. In contrast, ResNet50's poorer performance on the reduced dataset suggests that while it is a powerful model, its larger parameter count may make it harder to fine-tune effectively with very limited data, potentially requiring more data, more epochs, or a different fine-tuning strategy (e.g., unfreezing more layers) to achieve comparable results. For this specific task and dataset size, MobileNetV2 is clearly the more suitable and efficient choice.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Both MobileNetV2 and ResNet50 were trained on datasets reduced to 20 batches each (approximately 640 samples per dataset).\n",
        "*   **MobileNetV2 Results:**\n",
        "    *   Training Accuracy: \\~0.9973\n",
        "    *   Validation Accuracy: \\~0.9422\n",
        "    *   Test Accuracy: 0.9281\n",
        "    *   Epoch Training Time: \\~1 minute\n",
        "*   **ResNet50 Results:**\n",
        "    *   Training Accuracy: \\~0.4147\n",
        "    *   Validation Accuracy: \\~0.5375\n",
        "    *   Test Accuracy: 0.5719\n",
        "    *   Epoch Training Time: \\~4-5 minutes\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   MobileNetV2 is highly recommended for this image classification task, especially with limited data or computational resources, due to its superior balance of speed and accuracy compared to ResNet50 under these conditions.\n",
        "*   For ResNet50, further experimentation might include increasing the number of training epochs, implementing learning rate schedules, or exploring fine-tuning by unfreezing some of its top layers, though this would increase training time and require more careful hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1QQ-mtVMr9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221a6188"
      },
      "source": [
        "# Task\n",
        "Present the predicted class label, its confidence score, and compare it with the actual label of the sample image for the MobileNetV2 model's prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceb562a8"
      },
      "source": [
        "## Prepare Sample Image\n",
        "\n",
        "### Subtask:\n",
        "Ensure a sample image from the test dataset is prepared and formatted correctly for prediction (i.e., with a batch dimension). The `sample_image_expanded` and `sample_label` variables from the previous prediction task can be reused.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8787e4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Verify that the `sample_image_expanded` and `sample_label` variables are still available in the environment and correctly represent a prepared sample from the `test_ds` by re-printing their shapes and values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d556388",
        "outputId": "61bc7a4a-1681-4594-f904-dcf1dc0c6a97"
      },
      "source": [
        "print(f\"Verified expanded image shape: {sample_image_expanded.shape}\")\n",
        "print(f\"Verified sample label (raw integer): {sample_label.numpy()}\")\n",
        "print(f\"Verified sample label (class name): {class_names[sample_label.numpy()]}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified expanded image shape: (1, 224, 224, 3)\n",
            "Verified sample label (raw integer): 0\n",
            "Verified sample label (class name): 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ba5a67"
      },
      "source": [
        "## Make MobileNetV2 Prediction\n",
        "\n",
        "### Subtask:\n",
        "Use the `model_mobilenet.predict()` method on the prepared `sample_image_expanded` to obtain the model's probability predictions for each class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35892fc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires using the `model_mobilenet.predict()` method on the prepared `sample_image_expanded` and storing the results in `mobilenet_predictions`, then printing them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ce0b04",
        "outputId": "6368d533-a441-4ca8-b82c-65a96fa5e8b4"
      },
      "source": [
        "mobilenet_predictions = model_mobilenet.predict(sample_image_expanded)\n",
        "\n",
        "print(\"MobileNetV2 Model Predictions (probabilities for each class):\")\n",
        "print(mobilenet_predictions)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "MobileNetV2 Model Predictions (probabilities for each class):\n",
            "[[9.9972612e-01 3.4439898e-05 1.9286298e-07 2.3923085e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6199613"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully generated the probability predictions. Now, I need to interpret these predictions to find the predicted class, its confidence score, and compare it with the actual label, as per the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82d7eb5b",
        "outputId": "a44a2c28-42eb-4fcb-a18b-28e9154aed18"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Find the index of the class with the highest probability\n",
        "mobilenet_predicted_class_index = np.argmax(mobilenet_predictions[0])\n",
        "\n",
        "# 2. Extract the confidence score (the maximum probability)\n",
        "mobilenet_confidence_score = mobilenet_predictions[0][mobilenet_predicted_class_index]\n",
        "\n",
        "# 3. Use the class_names list to get the human-readable predicted class label\n",
        "mobilenet_predicted_class_name = class_names[mobilenet_predicted_class_index]\n",
        "\n",
        "# 4. Print the predicted class label and its corresponding confidence score\n",
        "print(f\"\\nMobileNetV2 Predicted Class Index: {mobilenet_predicted_class_index}\")\n",
        "print(f\"MobileNetV2 Predicted Class: {mobilenet_predicted_class_name} (Confidence: {mobilenet_confidence_score:.4f})\")\n",
        "print(f\"Actual Class: {class_names[sample_label.numpy()]}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MobileNetV2 Predicted Class Index: 0\n",
            "MobileNetV2 Predicted Class: 100 (Confidence: 0.9997)\n",
            "Actual Class: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22b2f8c3"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the predicted class label, its confidence score, and compare it with the actual label of the sample image for the MobileNetV2 model's prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9f40ed"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The predicted class label for the sample image using the MobileNetV2 model is '100' with a high confidence score of 0.9997. This prediction matches the actual label of the sample image, which is also '100'.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The sample image was successfully prepared and formatted, ensuring it had the correct shape of (1, 224, 224, 3) for model input, including a batch dimension.\n",
        "*   The actual label for the sample image was identified as '100'.\n",
        "*   The MobileNetV2 model predicted the class '100' for the sample image.\n",
        "*   The confidence score associated with this prediction was very high, at 0.9997 (or 99.97%).\n",
        "*   The model's prediction ('100') perfectly matched the actual label ('100') for the sample image.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The MobileNetV2 model demonstrated strong performance by accurately classifying the sample image with very high confidence.\n",
        "*   Further evaluation on a larger test set is recommended to confirm this high accuracy across various samples and classes.\n"
      ]
    }
  ]
}