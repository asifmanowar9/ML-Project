{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZz9j9Y2NYpR8mOjYo7Ip1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifmanowar9/BanglaTaka/blob/main/Code/BanglaTaka_(Asif)MobileNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz8KLqsJ1vxW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9014178b"
      },
      "source": [
        "# Task\n",
        "Build and evaluate a Convolutional Neural Network (CNN) model to classify banknote images, using the dataset from the Google Drive folder located at 'https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA'. The task involves accessing the data, preparing it, splitting it into training, validation, and test sets, building and training the CNN model, evaluating its performance with relevant metrics, and providing a summary of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1338d51"
      },
      "source": [
        "## Access Dataset from Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to access the dataset folder and list its contents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "215df033"
      },
      "source": [
        "### Step 1: Mount Google Drive and Access Dataset Folder\n",
        "\n",
        "To access the dataset, you'll first need to mount your Google Drive to this Colab environment.\n",
        "\n",
        "**Important Note:** The provided Google Drive link `https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA` points to a shared folder. To make it accessible from your mounted Google Drive, you **must create a shortcut to this shared folder in your 'My Drive'**.\n",
        "\n",
        "For example, if you create a shortcut named `DSA_Project_Folder` in your 'My Drive', its path after mounting will typically be `/content/drive/MyDrive/DSA_Project_Folder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f40e52e1"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide the Python code to mount Google Drive, which is the first step in accessing the dataset as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4227041",
        "outputId": "24c6b4ec-2fe8-438b-d6b2-bcbaacb809d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84043e4a"
      },
      "source": [
        "### Step 2: List Contents of the Shared Dataset Folder\n",
        "\n",
        "To confirm that you can access the dataset and understand its structure, you need to list the contents of the shared Google Drive folder. Remember, you should have created a shortcut to the provided shared folder (`https://drive.google.com/drive/folders/13P5Soos4thSeu9Su62lHVENzrSF3kxCA`) in your 'My Drive'.\n",
        "\n",
        "Replace `Your_Shortcut_Name` in the code below with the actual name of the shortcut you created (e.g., `DSA_Project_Folder`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8170281e"
      },
      "source": [
        "**Reasoning**:\n",
        "To allow the user to list the contents of the mounted Google Drive folder, I will provide a Python code block with a placeholder for their specific shortcut name. This directly addresses the second instruction of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d02c7c1",
        "outputId": "123238f1-8c3d-4b25-8da6-21246d04efc8"
      },
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/aa/dataset' # Replace 'Your_Shortcut_Name' with the actual name of your shortcut\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    print(f\"Listing contents of '{drive_path}':\")\n",
        "    for item in os.listdir(drive_path):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"Error: The path '{drive_path}' does not exist. Please check your shortcut name and path.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of '/content/drive/MyDrive/aa/dataset':\n",
            "100\n",
            "200\n",
            "500\n",
            "1000\n",
            "50\n",
            "2\n",
            "5\n",
            "10\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7584f8af"
      },
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Load the banknote images and their corresponding labels from the Google Drive folder, perform initial exploration, and preprocess the images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f975b232"
      },
      "source": [
        "**Reasoning**:\n",
        "The user provided detailed instructions for loading, exploring, and preprocessing the image dataset. I will generate a code block that imports TensorFlow, defines image dimensions and batch size, sets the corrected dataset root path, loads the dataset, prints class names, defines and applies a normalization function, and verifies the preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "531b65ec",
        "outputId": "40b22f31-31aa-44db-caa4-7e3fa7c0d1c9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Set the dataset_root variable to the confirmed path\n",
        "dataset_root = '/content/drive/MyDrive/aa/dataset'\n",
        "\n",
        "print(f\"Attempting to load dataset from: {dataset_root}\")\n",
        "\n",
        "try:\n",
        "    # 3. Load the image dataset using image_dataset_from_directory\n",
        "    raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_root,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(img_height, img_width),\n",
        "        interpolation='nearest',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # 4. Print class names and approximate number of images\n",
        "    class_names = raw_train_ds.class_names\n",
        "    print(f\"\\nClasses found: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "    # Corrected: Handle cardinality carefully\n",
        "    total_batches_cardinality = tf.data.experimental.cardinality(raw_train_ds)\n",
        "    if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "        print(\"Total number of batches could not be determined (tf.data.AUTOTUNE). Dataset size might be very large.\")\n",
        "    else:\n",
        "        # Ensure we call .numpy() only if it's a TensorFlow Tensor\n",
        "        total_batches = total_batches_cardinality.numpy() if hasattr(total_batches_cardinality, 'numpy') else total_batches_cardinality\n",
        "        total_images = total_batches * batch_size\n",
        "        print(f\"Approximate total number of images in the dataset: {total_images}\")\n",
        "\n",
        "    # 5. Preprocessing function for normalization (corrected)\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image, tf.float32) / 255.0 # Cast image to float32 first, then divide\n",
        "        return image, label\n",
        "\n",
        "    # 6. Apply preprocessing to the dataset\n",
        "    processed_train_ds = raw_train_ds.map(preprocess)\n",
        "    print(\"\\nImage normalization preprocessing applied to the dataset.\")\n",
        "\n",
        "    # 7. Verify preprocessing by taking one batch\n",
        "    for image_batch, labels_batch in processed_train_ds.take(1):\n",
        "        print(f\"\\nShape of one image batch after preprocessing: {image_batch.shape}\")\n",
        "        print(f\"Min pixel value in one image batch: {tf.reduce_min(image_batch)}\")\n",
        "        print(f\"Max pixel value in one image batch: {tf.reduce_max(image_batch)}\")\n",
        "        print(f\"Shape of one label batch: {labels_batch.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the 'dataset_root' is correct and points to a directory containing image subfolders (classes).\")\n",
        "    print(\"Example: '/content/drive/MyDrive/DSA_Project_Folder'\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: /content/drive/MyDrive/aa/dataset\n",
            "Found 10634 files belonging to 9 classes.\n",
            "\n",
            "Classes found: ['10', '100', '1000', '2', '20', '200', '5', '50', '500']\n",
            "Number of classes: 9\n",
            "Approximate total number of images in the dataset: 10656\n",
            "\n",
            "Image normalization preprocessing applied to the dataset.\n",
            "\n",
            "Shape of one image batch after preprocessing: (32, 128, 128, 3)\n",
            "Min pixel value in one image batch: 0.0\n",
            "Max pixel value in one image batch: 1.0\n",
            "Shape of one label batch: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26b2dc55"
      },
      "source": [
        "## Split Dataset\n",
        "\n",
        "### Subtask:\n",
        "Split the prepared dataset into training, validation, and test sets to properly evaluate the model's generalization capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8457cb76"
      },
      "source": [
        "### Step 4: Split the Dataset into Training, Validation, and Test Sets\n",
        "\n",
        "Now that the dataset is loaded and preprocessed, the next crucial step is to split it into training, validation, and test sets. This ensures that the model is trained on one part of the data, fine-tuned on another (validation), and finally evaluated on unseen data (test set) to provide an unbiased assessment of its performance.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Determine Total Images**: We'll use the `total_images` variable calculated in the previous step as an approximation for the dataset size.\n",
        "2.  **Define Split Ratios**: We will use a standard split of 70% for training, 15% for validation, and 15% for testing.\n",
        "3.  **Calculate Batches**: The dataset is already batched, so we will determine the number of batches for each split based on the ratios and the overall number of batches.\n",
        "4.  **Create Subsets**: TensorFlow's `tf.data.Dataset` API provides efficient methods (`take`, `skip`) for creating these subsets.\n",
        "5.  **Optimize Performance**: Apply `.cache()` and `.prefetch()` to each dataset split to improve training efficiency. `.cache()` keeps images in memory after the first epoch, and `.prefetch()` overlaps data preprocessing and model execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8b2691"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide the Python code to split the `processed_train_ds` into training, validation, and test sets according to the specified ratios, calculate the number of batches for each split, and apply performance optimizations like `.cache()` and `.prefetch()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af011d0f",
        "outputId": "cb5b1661-53c4-4146-efaf-3d439cf07dd8"
      },
      "source": [
        "import math\n",
        "\n",
        "# Assuming `processed_train_ds` and `batch_size` are available from previous steps\n",
        "# `total_images` was also calculated, but it's more robust to calculate based on `processed_train_ds` cardinality if possible.\n",
        "# Let's re-evaluate total batches to be sure, assuming `raw_train_ds` is the source.\n",
        "\n",
        "# Get the total number of batches from the processed dataset\n",
        "total_batches_cardinality = tf.data.experimental.cardinality(processed_train_ds)\n",
        "if total_batches_cardinality == tf.data.AUTOTUNE:\n",
        "    # Fallback to the approximate total_images if cardinality is not concrete\n",
        "    if 'total_images' in locals():\n",
        "        total_images_approx = total_images # Use previously calculated approximate total_images\n",
        "        total_batches_approx = math.ceil(total_images_approx / batch_size)\n",
        "    else:\n",
        "        print(\"Warning: Cannot determine total number of batches or images for splitting. Please ensure 'processed_train_ds' is properly formed.\")\n",
        "        # For demonstration, assign a default value or raise an error\n",
        "        total_batches_approx = 100 # Placeholder for a real scenario, this would cause issues\n",
        "else:\n",
        "    total_batches_approx = total_batches_cardinality.numpy() # Convert Tensor to Python int\n",
        "\n",
        "print(f\"Total batches in processed dataset: {total_batches_approx}\")\n",
        "\n",
        "# Define split ratios\n",
        "train_split_ratio = 0.7\n",
        "val_split_ratio = 0.15\n",
        "test_split_ratio = 0.15\n",
        "\n",
        "# Calculate number of batches for each split\n",
        "train_batches = int(total_batches_approx * train_split_ratio)\n",
        "val_batches = int(total_batches_approx * val_split_ratio)\n",
        "\n",
        "# The remaining batches go to the test set\n",
        "test_batches = total_batches_approx - train_batches - val_batches\n",
        "\n",
        "print(f\"\\nSplitting dataset:\")\n",
        "print(f\"  Training batches: {train_batches}\")\n",
        "print(f\"  Validation batches: {val_batches}\")\n",
        "print(f\"  Test batches: {test_batches}\")\n",
        "\n",
        "# Create the training, validation, and test datasets\n",
        "train_ds = processed_train_ds.take(train_batches)\n",
        "val_ds = processed_train_ds.skip(train_batches).take(val_batches)\n",
        "test_ds = processed_train_ds.skip(train_batches).skip(val_batches)\n",
        "\n",
        "# Verify the number of batches in each split (optional, for debugging)\n",
        "print(f\"\\nBatches after splitting:\")\n",
        "print(f\"  Train dataset batches (actual): {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"  Validation dataset batches (actual): {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(f\"  Test dataset batches (actual): {tf.data.experimental.cardinality(test_ds).numpy()}\")\n",
        "\n",
        "# Optimize performance with .cache() and .prefetch()\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"\\nDatasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches in processed dataset: 333\n",
            "\n",
            "Splitting dataset:\n",
            "  Training batches: 233\n",
            "  Validation batches: 49\n",
            "  Test batches: 51\n",
            "\n",
            "Batches after splitting:\n",
            "  Train dataset batches (actual): 233\n",
            "  Validation dataset batches (actual): 49\n",
            "  Test dataset batches (actual): 51\n",
            "\n",
            "Datasets (train_ds, val_ds, test_ds) created and optimized with .cache() and .prefetch().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759bbd0c"
      },
      "source": [
        "## Build and Compile CNN Model\n",
        "\n",
        "### Subtask:\n",
        "Build a Convolutional Neural Network (CNN) model suitable for image classification, including custom classification layers, and compile it with an appropriate optimizer, loss function, and metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba43a14"
      },
      "source": [
        "### Step 5: Build and Compile the CNN Model\n",
        "\n",
        "Now we will construct the Convolutional Neural Network (CNN) model. We'll leverage transfer learning by using a pre-trained base model (MobileNetV2) and then add custom classification layers on top. Finally, we'll compile the model with suitable settings for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87345084"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the detailed instructions, I will generate Python code to build and compile the CNN model using a pre-trained MobileNetV2 base, add custom classification layers, and then compile it with the specified optimizer, loss function, and metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "# Assuming img_height, img_width, and class_names are defined from previous steps\n",
        "# img_height = 128\n",
        "# img_width = 128\n",
        "# class_names = ['10', '100', '1000', '2', '20', '200', '5', '50', '500'] # Example\n",
        "\n",
        "print(f\"Image dimensions: ({img_height}, {img_width})\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# 1. Load a pre-trained base model (ResNet50) without its top classification layer\n",
        "base_model = ResNet50(input_shape=(img_height, img_width, 3),\n",
        "                      include_top=False,\n",
        "                      weights='imagenet')\n",
        "\n",
        "# 2. Freeze the base model's layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create a new Sequential model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(), # Converts feature maps to a single vector\n",
        "    Dense(128, activation='relu'), # A hidden dense layer\n",
        "    Dense(len(class_names), activation='softmax') # Output layer for multi-class classification\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Print a summary of the model\n",
        "print(\"\\nModel Architecture Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCNN model built and compiled successfully with ResNet50.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "WUZlJHVx4kyn",
        "outputId": "02fac942-3ab4-4ea1-cbcd-e161e8122e29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dimensions: (128, 128)\n",
            "Number of classes: 9\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Model Architecture Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,145\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,145</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,433\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,433</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN model built and compiled successfully with ResNet50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05d5175"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled CNN model using the training dataset and validate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a92f7f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training run was interrupted, so I will provide the code to re-run the model training with the specified datasets and store the training history, adhering to the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4196bf8",
        "outputId": "77d0df65-e934-40cd-f052-980cff59029c"
      },
      "source": [
        "epochs = 10 # Define the number of training epochs\n",
        "\n",
        "print(f\"\\nStarting model training for {epochs} epochs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"\\nModel training completed.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training for 10 epochs...\n",
            "Epoch 1/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 493ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.9943 - val_loss: 0.0171\n",
            "Epoch 2/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 512ms/step - accuracy: 0.9991 - loss: 0.0043 - val_accuracy: 0.9949 - val_loss: 0.0119\n",
            "Epoch 3/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 493ms/step - accuracy: 0.9999 - loss: 0.0012 - val_accuracy: 0.9962 - val_loss: 0.0097\n",
            "Epoch 4/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 492ms/step - accuracy: 1.0000 - loss: 4.8947e-04 - val_accuracy: 0.9955 - val_loss: 0.0096\n",
            "Epoch 5/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 498ms/step - accuracy: 1.0000 - loss: 2.7340e-04 - val_accuracy: 0.9955 - val_loss: 0.0102\n",
            "Epoch 6/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 486ms/step - accuracy: 1.0000 - loss: 2.0595e-04 - val_accuracy: 0.9962 - val_loss: 0.0102\n",
            "Epoch 7/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 488ms/step - accuracy: 1.0000 - loss: 1.6479e-04 - val_accuracy: 0.9962 - val_loss: 0.0101\n",
            "Epoch 8/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 505ms/step - accuracy: 1.0000 - loss: 1.3523e-04 - val_accuracy: 0.9962 - val_loss: 0.0100\n",
            "Epoch 9/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 525ms/step - accuracy: 1.0000 - loss: 1.1286e-04 - val_accuracy: 0.9962 - val_loss: 0.0099\n",
            "Epoch 10/10\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 499ms/step - accuracy: 1.0000 - loss: 9.5336e-05 - val_accuracy: 0.9962 - val_loss: 0.0099\n",
            "\n",
            "Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/mobilenetv2_trained_model_banglataka.keras')"
      ],
      "metadata": {
        "id": "D-JYtsJnAxSl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##load from the saved trained model"
      ],
      "metadata": {
        "id": "HQ8kG9T32k-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"/content/drive/MyDrive/mobilenetv2_trained_model_banglataka.keras\"\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "g_gYxm9k3V-3",
        "outputId": "1634a9ba-dc03-4f66-ec0c-a646083f2b22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,753,373\u001b[0m (10.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,753,373</span> (10.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m165,129\u001b[0m (645.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">165,129</span> (645.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m330,260\u001b[0m (1.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,260</span> (1.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81683bc"
      },
      "source": [
        "# Task\n",
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on the unseen test dataset. Calculate key metrics such as accuracy, precision, recall, F1-score, and generate a confusion matrix to understand the model's strengths and weaknesses in classifying different banknote denominations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee9d64a0"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on the unseen test dataset. Calculate key metrics such as accuracy, precision, recall, F1-score, and generate a confusion matrix to understand the model's strengths and weaknesses in classifying different banknote denominations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36aac3c5"
      },
      "source": [
        "### Step 7: Evaluate Model Performance on the Test Set\n",
        "\n",
        "Now that the model has been trained, it's crucial to evaluate its performance on an unseen test dataset. This step involves calculating standard classification metrics such as accuracy, precision, recall, and F1-score. Additionally, we will generate and visualize a confusion matrix to gain deeper insights into how well the model classifies each banknote denomination and to identify any specific classes it struggles with.\n",
        "\n",
        "**Instructions:**\n",
        "1. Evaluate the `model` on the `test_ds` to get the test loss and test accuracy. Print these values.\n",
        "2. Get the predictions (probabilities) for the `test_ds` using `model.predict()`. Store these in a variable, for example, `y_pred_probs`.\n",
        "3. Convert the predicted probabilities `y_pred_probs` into predicted class labels by taking the `argmax` along the appropriate axis. Store these as `y_pred`.\n",
        "4. Extract the true labels from the `test_ds`. This can be done by iterating through the `test_ds` to collect all labels. Store these as `y_true`.\n",
        "5. Import `classification_report` from `sklearn.metrics` and `numpy` as `np`. Use `classification_report` to print the precision, recall, and F1-score for each class, as well as overall averages.\n",
        "6. Import `confusion_matrix` from `sklearn.metrics` and `seaborn` as `sns`. Generate the confusion matrix using `confusion_matrix(y_true, y_pred)`.\n",
        "7. Plot the confusion matrix as a heatmap using `sns.heatmap`. Ensure the plot is labeled with class names, includes annotations, and has a title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cb53d74"
      },
      "source": [
        "**Reasoning**:\n",
        "I will provide the Python code to evaluate the trained model on the test set, calculate performance metrics (accuracy, precision, recall, F1-score), and visualize the confusion matrix as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490940e7",
        "outputId": "7c0a7a33-fffc-435e-9595-66e95cfc2d0e"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Evaluate the model on the test_ds\n",
        "print(\"\\nEvaluating model on the test set...\")\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 2. Get predictions (probabilities) for the test_ds\n",
        "print(\"\\nGenerating predictions for the test set...\")\n",
        "y_pred_probs = model.predict(test_ds)\n",
        "\n",
        "# 3. Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# 4. Extract true labels from the test_ds\n",
        "print(\"Extracting true labels from the test set...\")\n",
        "y_true = []\n",
        "for _, labels in test_ds:\n",
        "    y_true.extend(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Ensure y_true and y_pred have the same number of samples\n",
        "# The test_ds might have a final batch smaller than batch_size\n",
        "# We take min length to avoid index out of bounds if there's a discrepancy\n",
        "min_len = min(len(y_true), len(y_pred))\n",
        "y_true = y_true[:min_len]\n",
        "y_pred = y_pred[:min_len]\n",
        "\n",
        "# 5. Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "# class_names should be available from previous steps\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "# 6. Generate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# 7. Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nModel evaluation completed. Test loss, accuracy, classification report, and confusion matrix are displayed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on the test set...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "e17fda37",
        "outputId": "a837eb88-0f0c-416a-b929-99167def6f4b"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Evaluate the model on the test_ds\n",
        "print(\"\\nEvaluating model on the test set...\")\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 2. Get predictions (probabilities) for the test_ds\n",
        "print(\"\\nGenerating predictions for the test set...\")\n",
        "y_pred_probs = model.predict(test_ds)\n",
        "\n",
        "# 3. Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# 4. Extract true labels from the test_ds\n",
        "print(\"Extracting true labels from the test set...\")\n",
        "y_true = []\n",
        "for _, labels in test_ds:\n",
        "    y_true.extend(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Ensure y_true and y_pred have the same number of samples\n",
        "# The test_ds might have a final batch smaller than batch_size\n",
        "# We take min length to avoid index out of bounds if there's a discrepancy\n",
        "min_len = min(len(y_true), len(y_pred))\n",
        "y_true = y_true[:min_len]\n",
        "y_pred = y_pred[:min_len]\n",
        "\n",
        "# 5. Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "# class_names should be available from previous steps\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "# 6. Generate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# 7. Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nModel evaluation completed. Test loss, accuracy, classification report, and confusion matrix are displayed.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on the test set...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1702533077.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. Evaluate the model on the test_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating model on the test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dbu23eMj7Pgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}